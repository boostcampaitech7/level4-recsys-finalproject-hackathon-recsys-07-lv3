{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader에서 셔플사용하고 num_worker가 하나 보다 많을 때, 각각의 워커에 시드 설정을 해주지 않으면 재현성 보장이 안됨\n",
    "def worker_init_fn(worker_id, seed):\n",
    "    np.random.seed(seed + worker_id)\n",
    "    random.seed(seed + worker_id)\n",
    "\n",
    "worker_init_fn_with_seed = partial(worker_init_fn, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/ephemeral/home/data/cold/'\n",
    "save_dir = '../saved/'\n",
    "model_path = save_dir + 'ncf.pth'\n",
    "\n",
    "# Used to create reindex mapping dicts\n",
    "full_df = pd.read_csv(path + 'full.csv')\n",
    "# Used as training data\n",
    "train_df = pd.read_csv(path + 'train.csv')\n",
    "\n",
    "# Used for masking during Top K recommendation\n",
    "val_k = pd.read_csv(path + 'val_k.csv')\n",
    "test_k = pd.read_csv(path + 'test_k.csv')\n",
    "\n",
    "# Used as ground truth for Top K recommendation\n",
    "val_n = pd.read_csv(path + 'val_n.csv')\n",
    "test_n = pd.read_csv(path + 'test_n.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindex dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = full_df['user_id'].unique()\n",
    "items = full_df['item_id'].unique()\n",
    "\n",
    "user2idx = {user: i for i, user in enumerate(users)}\n",
    "item2idx = {item: i for i, item in enumerate(items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_df(df, user_mapping, item_mapping):\n",
    "    df['user_id'] = df['user_id'].map(user_mapping)\n",
    "    df['item_id'] = df['item_id'].map(item_mapping)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex dataframes\n",
    "reindex_df(train_df, user2idx, item2idx)\n",
    "reindex_df(val_k, user2idx, item2idx)\n",
    "reindex_df(val_n, user2idx, item2idx)\n",
    "reindex_df(test_k, user2idx, item2idx)\n",
    "reindex_df(test_n, user2idx, item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full df의 유저 수, 아이템 수\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "del full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val, test 전체 유저 아이디 리스트\n",
    "val_users = val_k['user_id'].unique()\n",
    "test_users = test_k['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저 별 인터랙션 개수\n",
    "val_inter_counts = val_k.groupby('user_id')['item_id'].count()\n",
    "test_inter_counts = test_k.groupby('user_id')['item_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1~15-shot 유저 아이디 리스트\n",
    "val_1_users = val_inter_counts[val_inter_counts == 1].index.tolist()\n",
    "val_3_users = val_inter_counts[val_inter_counts == 3].index.tolist()\n",
    "val_5_users = val_inter_counts[val_inter_counts == 5].index.tolist()\n",
    "val_10_users = val_inter_counts[val_inter_counts == 10].index.tolist()\n",
    "val_15_users = val_inter_counts[val_inter_counts == 15].index.tolist()\n",
    "\n",
    "test_1_users = test_inter_counts[test_inter_counts == 1].index.tolist()\n",
    "test_3_users = test_inter_counts[test_inter_counts == 3].index.tolist()\n",
    "test_5_users = test_inter_counts[test_inter_counts == 5].index.tolist()\n",
    "test_10_users = test_inter_counts[test_inter_counts == 10].index.tolist()\n",
    "test_15_users = test_inter_counts[test_inter_counts == 15].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_user_groups = {\n",
    "    'users_1': val_1_users,\n",
    "    'users_3': val_3_users,\n",
    "    'users_5': val_5_users,\n",
    "    'users_10': val_10_users,\n",
    "    'users_15': val_15_users,\n",
    "}\n",
    "\n",
    "test_user_groups = {\n",
    "    'users_1': test_1_users,\n",
    "    'users_3': test_3_users,\n",
    "    'users_5': test_5_users,\n",
    "    'users_10': test_10_users,\n",
    "    'users_15': test_15_users,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {user_id: [item_ids]} 딕셔너리 생성, train 에서는 neg_sampling에서 마스킹으로, Top K 추천에서는 scores에 대한 마스킹으로 사용 됌\n",
    "train_user_train_items = {k: list(v['item_id'].values) for k, v in train_df.groupby('user_id')}\n",
    "val_user_train_items = {k: list(v['item_id'].values) for k, v in val_k.groupby('user_id')}\n",
    "test_user_train_items = {k: list(v['item_id'].values) for k, v in test_k.groupby('user_id')}\n",
    "\n",
    "# val, test 유저들에 대한 정답 아이템 리스트 딕셔너리\n",
    "val_actual = {k: list(v['item_id'].values) for k, v in val_n.groupby('user_id')}\n",
    "test_actual = {k: list(v['item_id'].values) for k, v in test_n.groupby('user_id')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_k, test_k, val_n, test_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coo_matrix(df, n_users, n_items):\n",
    "    # torch 텐서로 변환\n",
    "    user_id_tensor = torch.tensor(df['user_id'].values, dtype=torch.long)\n",
    "    item_id_tensor = torch.tensor(df['item_id'].values, dtype=torch.long)\n",
    "    label_tensor = torch.ones(len(df), dtype=torch.float32)\n",
    "\n",
    "    # COO 희소 텐서 생성\n",
    "    indices = torch.stack([user_id_tensor, item_id_tensor])\n",
    "    values = label_tensor\n",
    "    size = (n_users, n_items)  # 전체 유저 x 전체 아이템 크기로 지정\n",
    "\n",
    "    return torch.sparse_coo_tensor(indices, values, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coo = create_coo_matrix(train_df, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_values = train_df.groupby('item_id').size().to_numpy()\n",
    "epsilon = 1e-8\n",
    "pop_values_log = np.log(pop_values + epsilon)\n",
    "item_popularity = (pop_values_log - pop_values_log.min()) / (pop_values_log.max() - pop_values_log.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, coo, user_train_items, item_popularity=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_train_items (dict): 사용자별 상호작용 아이템 딕셔너리.\n",
    "            num_items (int): 전체 아이템 수.\n",
    "            n_negs (int): 한 샘플 당 네거티브 샘플 개수.\n",
    "            item_popularity (list or array): 인덱스에 따른 아이템 인기도 (0~1 범위).\n",
    "        \"\"\"\n",
    "        self.users, self.pos_items = coo._indices()\n",
    "        self.user_train_items = user_train_items\n",
    "        self.n_items = coo.shape[1]\n",
    "        self.n_inter = coo._values().shape[0]\n",
    "        self.item_popularity = item_popularity\n",
    "        \n",
    "        # 아이템 인기도가 주어졌다면, 전역에서 인기도 상위 30%에 해당하는 아이템 리스트를 생성\n",
    "        if self.item_popularity is not None:\n",
    "            # 상위 30%에 해당하는 임계값: 70번째 백분위수 이상\n",
    "            threshold_value = np.percentile(self.item_popularity, 70)\n",
    "            # 리스트로 변환해서 random.choice가 빠르게 작동하도록 함\n",
    "            self.top_popular_items = list(set(np.where(np.array(self.item_popularity) >= threshold_value)[0]))\n",
    "        else:\n",
    "            self.top_popular_items = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_inter\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.pos_items[idx]\n",
    "\n",
    "        if self.top_popular_items is not None:\n",
    "            neg_item = self._get_pop_neg_item(user.item())\n",
    "        else:  # No top_popular_items_list\n",
    "            neg_item = self._get_neg_item(user.item())\n",
    "\n",
    "        return user, pos_item, neg_item\n",
    "    \n",
    "    def _get_neg_item(self, user):\n",
    "        train_items = set(self.user_train_items[user])\n",
    "        \n",
    "        neg_item = torch.randint(0, self.n_items, (1,)).item()\n",
    "        while neg_item in train_items:\n",
    "            neg_item = torch.randint(0, self.n_items, (1,)).item()\n",
    "        return neg_item\n",
    "    \n",
    "    def _get_pop_neg_item(self, user):\n",
    "        train_items = set(self.user_train_items[user])\n",
    "\n",
    "        neg_item = random.choice(self.top_popular_items)\n",
    "        for _ in range(10):  # 11 번까지만 상위 30퍼 내에서 무작위 샘플링 시도\n",
    "            if neg_item in train_items:\n",
    "                neg_item = random.choice(self.top_popular_items)\n",
    "            else:\n",
    "                break\n",
    "        if neg_item in train_items:  # 11번 내에 샘플링 실패 시 전체 아이템에서 무작위 샘플링\n",
    "            while neg_item in train_items:\n",
    "                neg_item = torch.randint(0, self.n_items, (1,)).item()\n",
    "        return neg_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BPRDataset(train_coo, train_user_train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn_with_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.mlp = nn.Sequential(  # [batch_size, emb_dim * 2]\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(emb_dim * 2, emb_dim),  # [batch_size, emb_dim]\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(emb_dim, emb_dim // 2),  # [batch_size, emb_dim // 2]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim // 2, 1)  # [batch_size, 1]\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_normal_(self.user_emb.weight)\n",
    "        nn.init.xavier_normal_(self.item_emb.weight)\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_emb(user)\n",
    "        item_emb = self.item_emb(item)\n",
    "        concat = torch.cat([user_emb, item_emb], dim=-1)  # [batch_size, emb_dim * 2]\n",
    "        return self.mlp(concat).squeeze()  # [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPR loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPR 손실 함수\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pos_scores, neg_scores):\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(model, user_id, num_items, top_k=10, user_train_items=None):\n",
    "    \"\"\"\n",
    "    단일 유저에 대해 전체 아이템에 대한 스코어를 계산한 후,\n",
    "    이미 학습에 활용된 아이템(train_items)이 있을 경우 이를 마스킹(-무한대로 대체)하고,\n",
    "    bn.argpartition을 활용해 상위 top_k 아이템을 효율적으로 추출하는 함수.\n",
    "    \n",
    "    args:\n",
    "        model: user_id와 item_id의 텐서를 입력받아 스코어를 반환하는 추천 모델.\n",
    "        user_id (int): 추천을 위한 대상 유저 ID.\n",
    "        num_items (int): 전체 아이템의 개수.\n",
    "        top_k (int): 추천할 아이템 수.\n",
    "        train_items (list 또는 np.array, optional): 학습 시 활용된 해당 유저의 아이템 인덱스 리스트.\n",
    "    \n",
    "    return:\n",
    "        추천 아이템 인덱스 리스트 (정렬되어 있음)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # user_id를 전체 아이템 개수만큼 반복하여 텐서 생성\n",
    "    user_ids = torch.full((num_items,), user_id, dtype=torch.long).to(device)\n",
    "    # 모든 아이템의 인덱스 생성\n",
    "    item_ids = torch.arange(num_items, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(user_ids, item_ids)\n",
    "    \n",
    "    # train_items가 제공되면 해당 아이템의 스코어를 마스킹 처리 (-무한대값으로 대체)\n",
    "    if user_train_items is not None:\n",
    "        # torch indexing은 list나 array로도 발동됨\n",
    "        scores[user_train_items] = -float('inf')\n",
    "    \n",
    "    # GPU에 있을 경우 CPU로 옮긴 후 numpy 배열로 변환\n",
    "    scores_np = scores.cpu().numpy()\n",
    "    \n",
    "    # bottleneck의 argpartition을 사용하여 상위 top_k의 후보 인덱스를 추출\n",
    "    # 음수 부호를 취해 내림차순 정렬 효과를 냄.\n",
    "    candidate_indices = bn.argpartition(-scores_np, top_k-1)[:top_k]\n",
    "    \n",
    "    # argpartition은 정렬되어 있지 않으므로, 위 후보들에 대해 추가 정렬(내림차순) 수행\n",
    "    sorted_top_indices = candidate_indices[np.argsort(-scores_np[candidate_indices])]\n",
    "    \n",
    "    return sorted_top_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = set(predicted[:k])\n",
    "    return len(actual_set & predicted_at_k) / k\n",
    "\n",
    "# Same as recall at this experiment environment\n",
    "# def precision_at_k(actual, predicted, k):\n",
    "#     actual_set = set(actual)\n",
    "#     predicted_at_k = set(predicted[:k])\n",
    "#     return len(actual_set & predicted_at_k) / len(predicted_at_k)\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = predicted[:k]\n",
    "    dcg = sum([1 / np.log2(i + 2) for i, p in enumerate(predicted_at_k) if p in actual_set])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(actual_set), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def map_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_at_k = predicted[:k]\n",
    "    score = 0.0\n",
    "    hit_count = 0.0\n",
    "    for i, item in enumerate(predicted_at_k):\n",
    "        if item in actual_set:\n",
    "            hit_count += 1.0\n",
    "            score += hit_count / (i + 1)\n",
    "    return score / len(actual_set) if actual else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, users, user_groups, n_items, top_k, user_train_items, user_actual):\n",
    "    \"\"\"\n",
    "    모델의 평가를 위해 전체 아이템에 대한 추천 리스트를 생성하고\n",
    "    recall@k 등의 평가 지표를 계산하는 함수.\n",
    "    \n",
    "    Args:\n",
    "      model (nn.Module): 추천 스코어를 계산하는 모델.\n",
    "      users (iterable): 평가할 전체 유저 ID 리스트.\n",
    "      user_groups (dict): 1,3,5,10,15-shot 유저 ID 리스트 딕셔너리.\n",
    "      n_items (int): 전체 아이템의 개수.\n",
    "      top_k (int): 상위 추천 아이템 수 (예: 10).\n",
    "      user_train_items (dict): 각 유저마다 학습에 사용된 아이템 인덱스 리스트 – 평가 시 해당 아이템은 마스킹 처리함.\n",
    "      user_actual (dict): 각 유저의 실제 정답(ground truth) 아이템 리스트.\n",
    "    \n",
    "    Returns:\n",
    "      dict: 전체 유저와 1-shot, 3-shot, 5-shot 유저에 대한 평균 recall@k, ndcg@k, map@k 값을 포함하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드로 전환 (dropout, batchnorm 등 고정)\n",
    "    all_metrics = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    metrics_1 = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    metrics_3 = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    metrics_5 = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    metrics_10 = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    metrics_15 = {'recall': [], 'ndcg': [], 'map': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user in tqdm(users, desc=\"Evaluating\", leave=False):\n",
    "            # 각 유저에 recommend_items 함수 사용 (학습에 사용한 아이템은 마스킹)\n",
    "            recommendations = recommend_items(model, user, n_items, top_k, user_train_items[user])\n",
    "            # 추천 결과와 실제 정답(dict나 list)에 따라 메트릭 계산\n",
    "            recall = recall_at_k(user_actual[user], recommendations, top_k)\n",
    "            ndcg = ndcg_at_k(user_actual[user], recommendations, top_k)\n",
    "            map = map_at_k(user_actual[user], recommendations, top_k)\n",
    "\n",
    "            all_metrics['recall'].append(recall)\n",
    "            all_metrics['ndcg'].append(ndcg)\n",
    "            all_metrics['map'].append(map)\n",
    "\n",
    "            if user in user_groups['users_1']:\n",
    "                metrics_1['recall'].append(recall)\n",
    "                metrics_1['ndcg'].append(ndcg)\n",
    "                metrics_1['map'].append(map)\n",
    "            elif user in user_groups['users_3']:\n",
    "                metrics_3['recall'].append(recall)\n",
    "                metrics_3['ndcg'].append(ndcg)\n",
    "                metrics_3['map'].append(map)\n",
    "            elif user in user_groups['users_5']:\n",
    "                metrics_5['recall'].append(recall)\n",
    "                metrics_5['ndcg'].append(ndcg)\n",
    "                metrics_5['map'].append(map)\n",
    "            elif user in user_groups['users_10']:\n",
    "                metrics_10['recall'].append(recall)\n",
    "                metrics_10['ndcg'].append(ndcg)\n",
    "                metrics_10['map'].append(map)\n",
    "            elif user in user_groups['users_15']:\n",
    "                metrics_15['recall'].append(recall)\n",
    "                metrics_15['ndcg'].append(ndcg)\n",
    "                metrics_15['map'].append(map)\n",
    "\n",
    "    model.train()  # 평가 후 다시 학습 모드로 전환\n",
    "\n",
    "    def average_metrics(metrics):\n",
    "        return {k: sum(v)/ len(v) for k, v in metrics.items()}\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'all': average_metrics(all_metrics),\n",
    "        '1-shot': average_metrics(metrics_1),\n",
    "        '3-shot': average_metrics(metrics_3),\n",
    "        '5-shot': average_metrics(metrics_5),\n",
    "        '10-shot': average_metrics(metrics_10),\n",
    "        '15-shot': average_metrics(metrics_15),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(mode, top_k, metrics):\n",
    "    \"\"\"\n",
    "    주어진 메트릭을 형식에 맞춰 출력하는 함수.\n",
    "    \n",
    "    Args:\n",
    "    - mode (str): 'val', 'best val', 'test' 모드.\n",
    "    - top_k (int): 상위 추천 아이템 수 (예: 10).\n",
    "    - metrics (dict): 평가 메트릭 딕셔너리.\n",
    "    \"\"\"\n",
    "    print(f\"{mode.capitalize()} - All: Recall@{top_k}: {metrics['all']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['all']['ndcg']:.4f}, MAP@{top_k}: {metrics['all']['map']:.4f}\")\n",
    "\n",
    "    print(f\"{mode.capitalize()} - 1-shot: Recall@{top_k}: {metrics['1-shot']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['1-shot']['ndcg']:.4f}, MAP@{top_k}: {metrics['1-shot']['map']:.4f}\")\n",
    "\n",
    "    print(f\"{mode.capitalize()} - 3-shot: Recall@{top_k}: {metrics['3-shot']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['3-shot']['ndcg']:.4f}, MAP@{top_k}: {metrics['3-shot']['map']:.4f}\")\n",
    "\n",
    "    print(f\"{mode.capitalize()} - 5-shot: Recall@{top_k}: {metrics['5-shot']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['5-shot']['ndcg']:.4f}, MAP@{top_k}: {metrics['5-shot']['map']:.4f}\")\n",
    "    \n",
    "    print(f\"{mode.capitalize()} - 10-shot: Recall@{top_k}: {metrics['10-shot']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['10-shot']['ndcg']:.4f}, MAP@{top_k}: {metrics['10-shot']['map']:.4f}\")\n",
    "    \n",
    "    print(f\"{mode.capitalize()} - 15-shot: Recall@{top_k}: {metrics['15-shot']['recall']:.4f}, \"\n",
    "          f\"NDCG@{top_k}: {metrics['15-shot']['ndcg']:.4f}, MAP@{top_k}: {metrics['15-shot']['map']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "emb_dim = 64\n",
    "dropout = 0.2\n",
    "model = NCF(n_users, n_items, emb_dim, dropout).to(device)\n",
    "criterion = BPRLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12890/12890 [03:35<00:00, 59.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - All: Recall@10: 0.0380, NDCG@10: 0.0675, MAP@10: 0.0339\n",
      "Val - 1-shot: Recall@10: 0.0297, NDCG@10: 0.0624, MAP@10: 0.0319\n",
      "Val - 3-shot: Recall@10: 0.0283, NDCG@10: 0.0498, MAP@10: 0.0237\n",
      "Val - 5-shot: Recall@10: 0.0300, NDCG@10: 0.0509, MAP@10: 0.0243\n",
      "Val - 10-shot: Recall@10: 0.0472, NDCG@10: 0.0820, MAP@10: 0.0419\n",
      "Val - 15-shot: Recall@10: 0.0548, NDCG@10: 0.0924, MAP@10: 0.0476\n",
      "Current best epoch:1, Model saved to ../saved/ncf.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 1\n",
      "Best val - All: Recall@10: 0.0380, NDCG@10: 0.0675, MAP@10: 0.0339\n",
      "Best val - 1-shot: Recall@10: 0.0297, NDCG@10: 0.0624, MAP@10: 0.0319\n",
      "Best val - 3-shot: Recall@10: 0.0283, NDCG@10: 0.0498, MAP@10: 0.0237\n",
      "Best val - 5-shot: Recall@10: 0.0300, NDCG@10: 0.0509, MAP@10: 0.0243\n",
      "Best val - 10-shot: Recall@10: 0.0472, NDCG@10: 0.0820, MAP@10: 0.0419\n",
      "Best val - 15-shot: Recall@10: 0.0548, NDCG@10: 0.0924, MAP@10: 0.0476\n",
      "Test results from best epoch 1\n",
      "Test - All: Recall@10: 0.0369, NDCG@10: 0.0663, MAP@10: 0.0337\n",
      "Test - 1-shot: Recall@10: 0.0303, NDCG@10: 0.0651, MAP@10: 0.0342\n",
      "Test - 3-shot: Recall@10: 0.0285, NDCG@10: 0.0505, MAP@10: 0.0248\n",
      "Test - 5-shot: Recall@10: 0.0310, NDCG@10: 0.0548, MAP@10: 0.0274\n",
      "Test - 10-shot: Recall@10: 0.0447, NDCG@10: 0.0774, MAP@10: 0.0400\n",
      "Test - 15-shot: Recall@10: 0.0499, NDCG@10: 0.0836, MAP@10: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "valid_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "epochs_after_best = 0\n",
    "best_val_ndcg = None\n",
    "best_val_metrics = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for user, pos_item, neg_item in tqdm(train_loader, desc=f'Training Epoch {epoch+1}'):\n",
    "        user = user.to(device)\n",
    "        pos_item = pos_item.to(device)\n",
    "        neg_item = neg_item.to(device)\n",
    "\n",
    "        pos_scores = model(user, pos_item)\n",
    "        neg_scores = model(user, neg_item)\n",
    "\n",
    "        loss = criterion(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}')\n",
    "\n",
    " # 평가: val_users, n_items, top_k, 그리고 유저별 학습/정답 아이템 정보 사용\n",
    "    if (epoch + 1) % valid_interval == 0:\n",
    "        # Evaluate on validation users\n",
    "        val_metrics = evaluate(model, val_users, val_user_groups, n_items, top_k, val_user_train_items, val_actual)\n",
    "        print_metrics('val', top_k, val_metrics)\n",
    "\n",
    "        # Check if this is the best validation NDCG\n",
    "        if best_val_ndcg is None or val_metrics['all']['ndcg'] > best_val_ndcg:\n",
    "            best_val_ndcg = val_metrics['all']['ndcg']\n",
    "            best_val_metrics = val_metrics\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f'Current best epoch:{best_epoch}, Model saved to {model_path}')\n",
    "            epochs_after_best = 0  # Resetting counter\n",
    "        else:\n",
    "            epochs_after_best += 1\n",
    "        if epochs_after_best >= early_stop:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "# Evaluate on test users\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "test_metrics = evaluate(model, test_users, test_user_groups, n_items, top_k, test_user_train_items, test_actual)\n",
    "\n",
    "# Print best validation metrics\n",
    "print(f'Best epoch: {best_epoch}')\n",
    "print_metrics('best val', top_k, best_val_metrics)\n",
    "# Print test metrics\n",
    "print(f'Test results from best epoch {best_epoch}')\n",
    "print_metrics('test', top_k, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset = BPRDataset(train_coo, train_user_train_items, item_popularity=item_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_loader = DataLoader(finetune_dataset, batch_size=1024, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn_with_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../saved/ncf.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/12890 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "valid_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "epochs_after_best = 0\n",
    "best_val_ndcg = None\n",
    "best_val_metrics = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for user, pos_item, neg_item in tqdm(finetune_loader, desc=f'Training Epoch {epoch+1}'):\n",
    "        user = user.to(device)\n",
    "        pos_item = pos_item.to(device)\n",
    "        neg_item = neg_item.to(device)\n",
    "\n",
    "        pos_scores = model(user, pos_item)\n",
    "        neg_scores = model(user, neg_item)\n",
    "\n",
    "        loss = criterion(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(finetune_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}')\n",
    "\n",
    " # 평가: val_users, n_items, top_k, 그리고 유저별 학습/정답 아이템 정보 사용\n",
    "    if (epoch + 1) % valid_interval == 0:\n",
    "        # Evaluate on validation users\n",
    "        val_metrics = evaluate(model, val_users, val_user_groups, n_items, top_k, val_user_train_items, val_actual)\n",
    "        print_metrics('val', top_k, val_metrics)\n",
    "\n",
    "        # Check if this is the best validation NDCG\n",
    "        if best_val_ndcg is None or val_metrics['all']['ndcg'] > best_val_ndcg:\n",
    "            best_val_ndcg = val_metrics['all']['ndcg']\n",
    "            best_val_metrics = val_metrics\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f'Current best epoch:{best_epoch}, Model saved to {model_path}')\n",
    "            epochs_after_best = 0  # Resetting counter\n",
    "        else:\n",
    "            epochs_after_best += 1\n",
    "        if epochs_after_best >= early_stop:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "# Evaluate on test users\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "test_metrics = evaluate(model, test_users, test_user_groups, n_items, top_k, test_user_train_items, test_actual)\n",
    "\n",
    "# Print best validation metrics\n",
    "print(f'Best epoch: {best_epoch}')\n",
    "print_metrics('best val', top_k, best_val_metrics)\n",
    "# Print test metrics\n",
    "print(f'Test results from best epoch {best_epoch}')\n",
    "print_metrics('test', top_k, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - All: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Val - 1-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Val - 3-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Val - 5-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Val - 10-shot: Recall@10: 0.0000, NDCG@10: 0.0001, MAP@10: 0.0000\n",
      "Val - 15-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - All: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Test - 1-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Test - 3-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Test - 5-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n",
      "Test - 10-shot: Recall@10: 0.0000, NDCG@10: 0.0001, MAP@10: 0.0001\n",
      "Test - 15-shot: Recall@10: 0.0000, NDCG@10: 0.0000, MAP@10: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "val_metrics = evaluate(model, val_users, val_user_groups, n_items, top_k, val_user_train_items, val_actual)\n",
    "print_metrics('val', top_k, val_metrics)\n",
    "test_metrics = evaluate(model, test_users, test_user_groups, n_items, top_k, test_user_train_items, test_actual)\n",
    "print_metrics('test', top_k, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 유저에게 추천 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_emb): Embedding(126972, 64)\n",
       "  (item_emb): Embedding(24790, 64)\n",
       "  (mlp): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../saved/ncf.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12698/12698 [00:16<00:00, 767.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# 빈 리스트를 생성하여 추천 결과를 저장\n",
    "recommendations = []\n",
    "\n",
    "# 각 사용자에 대해 추천 아이템을 생성하고 리스트에 추가\n",
    "for user_id in tqdm(test_users):\n",
    "    recommended_items = recommend_items(model, user_id, n_items, 10, test_user_train_items[user_id])\n",
    "    for item_id in recommended_items:\n",
    "        recommendations.append({'user_id': user_id, 'item_id': item_id})\n",
    "\n",
    "# 리스트를 데이터프레임으로 변환\n",
    "result = pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2user = {v: k for k, v in user2idx.items()}\n",
    "idx2item = {v: k for k, v in item2idx.items()}\n",
    "\n",
    "result['user_id'] = result['user_id'].map(idx2user)\n",
    "result['item_id'] = result['item_id'].map(idx2item)\n",
    "\n",
    "result.to_csv('../saved/result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딕셔너리로 추천 결과 생성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_recommendations = {user_id: recommend_items(model, user_id, n_items, 10, test_user_train_items[user_id]) for user_id in tqdm(test_users)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.DataFrame([\n",
    "#     {'user_id': user, 'item_id': item}\n",
    "#     for user, items in test_recommendations.items()\n",
    "#     for item in items\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
